# awesome-training-free-methods
This is a repository to collect recent training-free algorithms, which can run on a low-end GPU card such as Tesla T4.

## Material Transfer

**ZeST: Zero-Shot Material Transfer from a Single Image.**<br>
*Ta-Ying Cheng, Prafull Sharma, Andrew Markham, Niki Trigoni, Varun Jampani.*<br>
arXiv 2024. [[PDF](https://arxiv.org/pdf/2404.06425)] [[Project](https://ttchengab.github.io/zest/)] [[Code]](https://github.com/ttchengab/zest_code)<br>

## Style Transfer

**Visual Style Prompting with Swapping Self-Attention.**<br>
*Jaeseok Jeong, Junho Kim, Yunjey Choi, Gayoung Lee, Youngjung Uh.*<br>
arXiv 2024. [[PDF](https://arxiv.org/pdf/2402.12974)] [[Project](https://curryjung.github.io/VisualStylePrompt/)] [[Code]](https://github.com/naver-ai/Visual-Style-Prompting)<br>

**FreeStyle: Free Lunch for Text-guided Style Transfer using Diffusion Models.**<br>
*Feihong He, Gang Li, Mengyuan Zhang, Leilei Yan, Lingyu Si, Fanzhang Li.*<br>
arXiv 2024. [[PDF](https://arxiv.org/pdf/2401.15636)] [[Project](https://freestylefreelunch.github.io/)] [[Code]](https://github.com/FreeStyleFreeLunch/FreeStyle)<br>

**Eye-for-an-eye: Appearance Transfer with Semantic Correspondence in Diffusion Models.**<br>
*Sooyeon Go, Kyungmook Choi, Minjung Shin, Youngjung Uh.*<br>
arXiv 2024. [[PDF](https://arxiv.org/pdf/2406.07008)] [[Project](https://sooyeon-go.github.io/eye_for_an_eye/)] [[Code]](https://github.com/sooyeon-go/eye_for_an_eye)<br>

**Ctrl-X: Controlling Structure and Appearance for Text-To-Image Generation Without Guidance.**<br>
*Kuan Heng Lin, Sicheng Mo, Ben Klingher, Fangzhou Mu, Bolei Zhou.*<br>
arXiv 2024. [[PDF](https://arxiv.org/pdf/2406.07540)] [[Project](https://genforce.github.io/ctrl-x/)] [[Code]](https://github.com/genforce/ctrl-x)<br>

**RB-Modulation: Training-Free Personalization of Diffusion Models using Stochastic Optimal Control.**<br>
*Litu Rout, Yujia Chen, Nataniel Ruiz, Abhishek Kumar, Constantine Caramanis, Sanjay Shakkottai, Wen-Sheng Chu.*<br>
arXiv 2024. [[PDF](https://arxiv.org/pdf/2405.17401)] [[Project](https://rb-modulation.github.io/)] [[Code]](https://github.com/LituRout/RB-Modulation)<br>

**Tuning-Free Adaptive Style Incorporation for Structure-Consistent Text-Driven Style Transfer.**<br>
*Yanqi Ge, Jiaqi Liu, Qingnan Fan, Xi Jiang, Ye Huang, Shuai Qin, Hong Gu, Wen Li, Lixin Duan.*<br>
arXiv 2024. [[PDF](https://arxiv.org/pdf/2404.06835.pdf)]<br>
Comments: It aims to achieve the style editing of the source image. Specifically, it divides the target prompt into a content prompt and style prompt, and then the cross-attention results of the two prompts are adaptively fused. Through this kind way, the proposed method could improve the style editing outcomes. It looks like PnP, particularly focusing on using style prompts.

**Style Injection in Diffusion: A Training-free Approach for Adapting Large-scale Diffusion Models for Style Transfer.**<br>
*Jiwoo Chung, Sangeek Hyun, Jae-Pil Heo.*<br>
CVPR 2024. [[PDF](https://arxiv.org/pdf/2312.09008.pdf)] [[Project](https://jiwoogit.github.io/StyleID_site/)] [[Code]](https://github.com/jiwoogit/StyleID)<br>

## Image Manipulation

**AccDiffusion: An Accurate Method for Higher-Resolution Image Generation.**<br>
*Zhihang Lin, Mingbao Lin, Meng Zhao, Rongrong Ji.*<br>
ECCV 2024. [[PDF](https://arxiv.org/abs/2407.10738v2)] [[Project](https://lzhxmu.github.io/accdiffusion/accdiffusion.html)] [[Code]](https://github.com/lzhxmu/AccDiffusion)<br>

**ResMaster: Mastering High-Resolution Image Generation via Structural and Fine-Grained Guidance.**<br>
*Shuwei Shi, Wenbo Li, Yuechen Zhang, Jingwen He, Biao Gong, Yinqiang Zheng.*<br>
arXiv 2024. [[PDF](https://arxiv.org/pdf/2406.16476)] [[Project](https://shuweis.github.io/ResMaster/)] [[Code]](https://github.com/Shuweis/ResMaster)<br>

**Coherent Zero-Shot Visual Instruction Generation.**<br>
*Quynh Phung, Songwei Ge, Jia-Bin Huang.*<br>
arXiv 2024. [[PDF](https://arxiv.org/pdf/2406.04337)] [[Project](https://instruct-vis-zero.github.io/)]<br>

**DiffUHaul: A Training-Free Method for Object Dragging in Images.**<br>
*Ganggui Ding, Canyu Zhao, Wen Wang, Zhen Yang, Zide Liu, Hao Chen, Chunhua Shen.*<br>
arXiv 2024. [[PDF](https://arxiv.org/pdf/2406.01594)] [[Project](https://omriavrahami.com/diffuhaul/)]<br>

**FreeCustom: Tuning-Free Customized Image Generation for Multi-Concept Composition.**<br>
*Ganggui Ding, Canyu Zhao, Wen Wang, Zhen Yang, Zide Liu, Hao Chen, Chunhua Shen.*<br>
CVPR 2024. [[PDF](https://arxiv.org/pdf/2405.13870)] [[Code]](https://github.com/aim-uofa/FreeCustom)<br>

**Analogist: Out-of-the-box Visual In-Context Learning with Image Diffusion Model.**<br>
*Zheng Gu, Shiyuan Yang, Jing Liao, Jing Huo, Yang Gao.*<br>
Siggraph 2024. [[PDF](https://arxiv.org/pdf/2405.10316)] [[Project](https://analogist2d.github.io/)] [[Code]](https://github.com/edward3862/Analogist)<br>

**Training-free Subject-Enhanced Attention Guidance for Compositional Text-to-image Generation.**<br>
*Shengyuan Liu, Bo Wang, Ye Ma, Te Yang, Xipeng Cao, Quan Chen, Han Li, Di Dong, Peng Jiang.*<br>
arXiv 2024. [[PDF](https://arxiv.org/pdf/2405.06948)]<br>

**ObjectAdd: Adding Objects into Image via a Training-Free Diffusion Modification Fashion.**<br>
*Ziyue Zhang, Mingbao Lin, Rongrong Ji.*<br>
arXiv 2024. [[PDF](https://arxiv.org/pdf/2404.17230)]<br>

**DemoFusion: Democratising High-Resolution Image Generation With No $$$.**<br>
*Ruoyi Du, Dongliang Chang, Timothy Hospedales, Yi-Zhe Song, Zhanyu Ma.*<br>
CVPR 2024. [[PDF](https://arxiv.org/pdf/2311.16973.pdf)] [[Project]](https://ruoyidu.github.io/demofusion/demofusion.html) [[Code]](https://github.com/PRIS-CV/DemoFusion)<br>

**HiDiffusion: Unlocking High-Resolution Creativity and Efficiency in Low-Resolution Trained Diffusion Models.**<br>
*Shen Zhang, Zhaowei Chen, Zhenyu Zhao, Zhenyuan Chen, Yao Tang, Yuhao Chen, Wengang Cao, Jiajun Liang.*<br>
arXiv 2024. [[PDF](https://arxiv.org/pdf/2311.17528.pdf)] [[Project]](https://hidiffusion.github.io/) [[Code]](https://github.com/megvii-research/HiDiffusion)<br>

**CutDiffusion: A Simple, Fast, Cheap, and Strong Diffusion Extrapolation Method.**<br>
*Mingbao Lin, Zhihang Lin, Wengyi Zhan, Liujuan Cao, Rongrong Ji.*<br>
arXiv 2024. [[PDF](https://arxiv.org/pdf/2404.15141.pdf)] [[Project]](https://github.com/lmbxmu/CutDiffusion)<br>

**FreeDiff: Progressive Frequency Truncation for Image Editing with Diffusion Models.**<br>
*Wei Wu, Qingnan Fan, Shuai Qin, Hong Gu, Ruoyu Zhao, Antoni B. Chan.*<br>
arXiv 2024. [[PDF](https://arxiv.org/pdf/2404.11895.pdf)] <br>

**Training-Free Consistent Text-to-Image Generation.**<br>
*Yoad Tewel, Omri Kaduri, Rinon Gal, Yoni Kasten, Lior Wolf, Gal Chechik, Yuval Atzmon.*<br>
arXiv 2024. [[PDF](https://arxiv.org/pdf/2402.03286.pdf)] [[Project]](https://consistory-paper.github.io/)<br>

**FouriScale: A Frequency Perspective on Training-Free High-Resolution Image Synthesis.**<br>
*Linjiang Huang, Rongyao Fang, Aiping Zhang, Guanglu Song, Si Liu, Yu Liu, Hongsheng Li.*<br>
arXiv 2024. [[PDF](https://arxiv.org/pdf/2403.12963.pdf)] [[Code]](https://github.com/LeonHLJ/FouriScale)<br>

**FreeControl: Training-Free Spatial Control of Any Text-to-Image Diffusion Model with Any Condition.**<br>
*Sicheng Mo, Fangzhou Mu, Kuan Heng Lin, Yanli Liu, Bochen Guan, Yin Li, Bolei Zhou.*<br>
CVPR 2024. [[PDF](https://arxiv.org/pdf/2312.07536.pdf)] [[Project](https://genforce.github.io/freecontrol/)] [[Code]](https://github.com/genforce/freecontrol)<br>

**Plug-and-Play Diffusion Features for Text-Driven Image-to-Image Translation.**<br>
*Narek Tumanyan, Michal Geyer, Shai Bagon, Tali Dekel.*<br>
CVPR 2023. [[PDF](https://arxiv.org/pdf/2211.12572.pdf)] [[Project](https://pnp-diffusion.github.io/)] [[Code]](https://github.com/MichalGeyer/plug-and-play)<br>

## Video Editing

**Slicedit: Zero-Shot Video Editing With Text-to-Image Diffusion Models Using Spatio-Temporal Slices.**<br>
*Nathaniel Cohen, Vladimir Kulikov, Matan Kleiner, Inbar Huberman-Spiegelglas, Tomer Michaeli.*<br>
ICML 2024. [[PDF](https://arxiv.org/pdf/2405.12211)] [[Project](https://matankleiner.github.io/slicedit/)] [[Code]](https://matankleiner.github.io/slicedit/)<br>

**FLATTEN: optical FLow-guided ATTENtion for consistent text-to-video editing.**<br>
*Yuren Cong, Mengmeng Xu, Christian Simon, Shoufa Chen, Jiawei Ren, Yanping Xie, Juan-Manuel Perez-Rua, Bodo Rosenhahn, Tao Xiang, Sen He.*<br>
ICLR 2024. [[PDF](https://arxiv.org/pdf/2310.05922.pdf)] [[Project](https://flatten-video-editing.github.io/)] [[Code]](https://github.com/yrcong/flatten)<br>

**TokenFlow: Consistent Diffusion Features for Consistent Video Editing.**<br>
*Michal Geyer, Omer Bar-Tal, Shai Bagon, Tali Dekel.*<br>
ICLR 2024. [[PDF](https://arxiv.org/pdf/2307.10373.pdf)] [[Project](https://diffusion-tokenflow.github.io/)] [[Code]](https://github.com/omerbt/TokenFlow)<br>




